# ArXiv AI Co-Scientist: Comprehensive Project Plan

## Executive Summary

**Project:** Build an open-source "Scientific Intelligence Engine" that discovers, interprets, categorizes, connects, and predicts scientific content from arXiv entirely cost-free.

**Architecture:** API-First (Semantic Scholar + Gemini)
**Status:** âœ… Phases 1-4 Complete (Migrated Jan 2026)
**Cost:** $0 (all free-tier APIs and open-source)
**Target Domain:** Physics & Mathematics (~1.4M papers)
**Initial Focus:** Quantum Physics (quant-ph) + Quantum Algebra (math.QA) - ~100k papers with dense cross-references

---

## 1. Architecture Decision

### 1.1 API-First vs Local-First Comparison

**Decision Made (Jan 7, 2026):** Pivot to API-First
**Rationale:** Superior data quality, reduced storage, machine-agnostic, zero infrastructure cost

| Dimension | API-First | Local-First |
|-----------|-----------|------------|
| **Data Source** | Semantic Scholar API | Kaggle 4.7GB dump |
| **Data Freshness** | Real-time | Static snapshot |
| **LLM** | Gemini 1.5 Flash/Pro | Ollama (Llama 3.2 8B) |
| **Storage** | ~8GB total | 10-70GB |
| **Citation Data** | Rich (context + influence) | Limited |
| **Cost** | $0/month | $0/month |
| **Platform** | Any machine | M1/M2 Mac required |
| **Author Profiles** | Yes (S2) | No |
| **AI TLDRs** | Yes (S2) | No (generate with LLM) |

**Key Advantages of API-First:**
- Eliminate 4.7GB Kaggle download + Ollama model weights
- Real-time data instead of stale snapshot
- Citation context (surrounding sentences) from S2
- Influence metrics (which citations are "highly influential")
- No Apple Silicon optimization required
- Gemini's 1M token context for complex reasoning

**Rate Limiting Strategy:**
- Semantic Scholar: 10 req/sec (free with API key), 1 req/sec (free tier)
- Gemini: 60 RPM (Flash) / 15 RPM (Pro) free tier
- 100k papers: ~33 hours processing overnight (0 cost)

### 1.2 Implementation Decision Log

| Date | Decision | Rationale | Impact |
|------|----------|-----------|--------|
| Jan 7 | API-First | Data quality + storage savings | Removed Kaggle, added S2 client |
| Jan 4 | Multi-Provider LLM | Fallback options + cost coverage | Gemini (primary), Groq, Ollama |
| Dec 20 | Neo4j Community | Self-hosted + APOC support | Graph storage finalized |
| Dec 18 | ChromaDB | Embedded vectors + zero config | Search index finalized |

---

## 2. Implementation Roadmap

### Phase 1: Foundation âœ… COMPLETE (Dec 2025 - Jan 7)

**Goal:** Basic data pipeline and storage setup

| Task | Status | Output |
|------|--------|--------|
| Monorepo scaffolding | âœ… Done | Poetry + packages structure |
| Semantic Scholar client | âœ… Done | `packages/ingestion/s2_client.py` (S2Client) |
| Neo4j schema | âœ… Done | Graph initialization in `packages/knowledge/neo4j_client.py` |
| ChromaDB setup | âœ… Done | Vector store in `packages/knowledge/chromadb_client.py` |
| Multi-provider LLM factory | âœ… Done | Gemini/Groq/Ollama factory in `packages/ai/factory.py` |
| CLI scaffolding | âœ… Done | Click commands in `apps/cli/main.py` |

**Deliverable:** Can fetch paper metadata and perform semantic analysis via APIs

**CLI Commands Implemented:**
- âœ… `fetch` - NEW: Fetch papers from S2 API by arXiv ID
- âœ… `search` - Semantic search via ChromaDB
- âœ… `summarize` - Summarize papers using LLM
- âœ… `extract` - Extract entities (methods, theorems, datasets)
- âœ… `check` - System health check
- âœ… `ai-check` - LLM/AI status
- âœ… `db-stats` - Database statistics

### Phase 2: PDF Parsing âœ… COMPLETE (Jan 9, 2026)

**Goal:** Production-grade PDF parsing for full-text analysis

| Task | Priority | Effort | Status |
|------|----------|--------|--------|
| Marker integration for complex PDFs | P0 | 1d | âœ… Done |
| Grobid integration for citations | P0 | 1d | âœ… Done |
| Semantic chunking by section | P0 | 1d | âœ… Done |
| LaTeX/math extraction | P1 | 1d | âœ… Done |
| Parsing pipeline orchestration | P0 | 1d | âœ… Done |
| Parsing quality metrics | P2 | 0.5d | âœ… Done |
| CLI commands (parse, validate) | P1 | 0.5d | â³ Pending |
| Comprehensive tests | P1 | 1d | â³ Pending |

**Deliverable:** âœ… Structured markdown with citations, equations, theorems from PDF full text

**Implementation Details:**
- **marker_parser.py**: High-quality PDFâ†’Markdown with LaTeX preservation (348 lines)
- **grobid_parser.py**: Citation extraction with TEI XML parsing (361 lines)
- **latex_extractor.py**: Equations, theorems, conjectures, constants (377 lines)
- **semantic_chunker.py**: Section-aware chunking with metadata (328 lines)
- **parsing_pipeline.py**: Full orchestration with fallback chain (358 lines)

**Parsing Pipeline (Physics/Math Optimized):**
```
PDF Input
â”œâ”€â–º Grobid â”€â”€â”€â”€â”€â”€â–º Metadata + Citations (fast, always run)
â”œâ”€â–º Marker â”€â”€â”€â”€â”€â”€â–º Full text with sections (primary)
â”‚   â””â”€â–º If slow/fails â”€â”€â–º PyMuPDF fallback
â””â”€â–º Validation â”€â”€â–º LaTeX verification + merging
```

**Key Challenge:** LaTeX-heavy physics papers require specialized handling
- Nougat/Marker for equation reconstruction
- Grobid for citation extraction
- Semantic chunking to preserve proof structure

### Phase 3: Knowledge Graph (NEXT)

**Goal:** Graph storage and hybrid search

| Task | Priority | Effort |
|------|----------|--------|
| Paper ingestion to Neo4j | P0 | 1d |
| Citation network construction | P0 | 1d |
| Vector index for papers | P0 | 0.5d |
| Hybrid search (vector + graph) | P1 | 1d |
| Graph query API | P1 | 0.5d |
| Index optimization | P2 | 0.5d |

**Deliverable:** Queryable knowledge graph with 1000+ papers, <500ms queries

**Data Model:**

**Paper Node:**
```cypher
CREATE (p:Paper {
  arxiv_id: "2401.12345",
  title: "...",
  abstract: "...",
  authors: ["Alice", "Bob"],
  categories: ["quant-ph", "math.QA"],
  published_date: "2024-01-23",
  s2_id: "...",
  embedding: [float[768]],
  tl_dr: "...",  // From Semantic Scholar
  summary: "..."  // LLM-generated
})
```

**Concept Node:**
```cypher
CREATE (c:Concept {
  name: "Quantum Error Correction",
  type: "Method|Algorithm|Dataset|Task|Constant",
  embedding: [float[768]],
  paper_count: int
})
```

**Citation Edge:**
```cypher
CREATE (p1)-[:CITES {
  intent: "Method|Background|Result|Critique|Extension",
  context: "Surrounding sentence...",
  position: "abstract|introduction|methods|results"
}]->(p2)
```

### Phase 4: AI Analysis âœ… COMPLETE (Jan 4 - Jan 7)

**Goal:** LLM-powered interpretation

| Task | Status |
|------|--------|
| Multi-provider LLM (Gemini, Groq, Ollama) | âœ… Done |
| Summarization prompts | âœ… Done |
| Entity extraction (methods, datasets, theorems) | âœ… Done |
| Citation intent classification | âœ… Done |
| Structured output parsing (Pydantic) | âœ… Done |
| LangChain orchestration | ğŸ”„ In Progress |
| Batch processing pipeline | â³ Pending |

**Deliverable:** Papers enriched with AI-generated metadata

**Entity Types Extracted:**
| Entity | Detection | Storage |
|--------|-----------|---------|
| **Theorem** | Regex + LLM | Concept node + linking |
| **Equation** | Named equation detection | Concept + LaTeX storage |
| **Constant** | Physics ontology lookup | Property on Concept |
| **Method** | LLM extraction | Concept node |
| **Dataset** | LLM + regex patterns | Concept node |
| **Conjecture** | Named conjecture database | Concept node |

**Implemented Modules:**
- `packages/ai/gemini_client.py` - Gemini API wrapper with structured output
- `packages/ai/groq_client.py` - Groq backup provider
- `packages/ai/ollama_client.py` - Local LLM fallback
- `packages/ai/factory.py` - Provider selection via LLM_PROVIDER env var
- `packages/ai/summarizer.py` - Multi-level summarization (brief/standard/detailed)
- `packages/ai/entity_extractor.py` - Entity extraction with Pydantic validation
- `packages/ai/citation_classifier.py` - Citation intent classification

### Phase 5: Predictions (NEXT)

**Goal:** Link prediction and hypothesis generation

| Task | Priority | Effort |
|------|----------|--------|
| GraphSAGE model training | P0 | 2d |
| Link prediction pipeline | P0 | 1d |
| Structural hole detection | P1 | 1d |
| Hypothesis generation agent | P1 | 2d |
| Prediction scoring API | P2 | 0.5d |

**Deliverable:** System predicts future connections, generates hypotheses

**Approach:**
- Train GraphSAGE on citation network (papers + concepts)
- Predict missing edges between papers sharing methods/datasets
- Detect "structural holes" (disconnect concepts that should be linked)
- Use Gemini agent to generate hypotheses from gaps

### Phase 6: Frontend (PLANNED)

**Goal:** Interactive visualization

| Task | Priority | Effort |
|------|----------|--------|
| FastAPI backend | P0 | 1d |
| React + Vite scaffolding | P0 | 0.5d |
| Sigma.js graph component | P0 | 2d |
| Search interface | P0 | 1d |
| Paper detail view | P1 | 1d |
| Cluster visualization | P1 | 1d |

**Deliverable:** Interactive web UI for exploration

### Phase 7: Production Polish (PLANNED)

**Goal:** Self-hostable, documented system

| Task | Priority | Effort |
|------|----------|--------|
| Docker Compose setup | P1 | 1d |
| OAI-PMH incremental sync | P1 | 1d |
| Error handling + retry logic | P1 | 1d |
| Performance optimization | P2 | 1d |
| Documentation | P2 | 1d |

**Deliverable:** Production-ready, self-hosted deployment

---

## 3. Technology Stack

### Core Technologies

| Layer | Technology | Purpose | Free Tier |
|-------|-----------|---------|-----------|
| **Language** | Python 3.11+ | Type-safe ML pipeline | N/A |
| **API: Data** | Semantic Scholar API | Paper metadata + citations | 10 req/sec |
| **API: LLM** | Gemini 1.5 Flash/Pro | Semantic analysis | 60 RPM / 1M tokens/day |
| **API: Fallback** | Groq API | Alternative LLM | 3000 req/month free |
| **API: Local** | Ollama | Local LLM fallback | Free (self-hosted) |
| **Database: Graph** | Neo4j Community | Knowledge graph | Free self-hosted |
| **Database: Vector** | ChromaDB | Semantic search | Free embedded |
| **Embeddings** | sentence-transformers (all-mpnet-base-v2) | Text encoding | Free (local) |
| **ML: GNN** | PyTorch Geometric | Link prediction | Free (local) |
| **Orchestration** | LangChain | Agent workflows | Free (local) |
| **PDF Parsing** | PyMuPDF + Marker + Grobid | Text extraction | Free (local) |
| **Web API** | FastAPI | REST/GraphQL backend | Free (local) |
| **Frontend** | React + Vite + Sigma.js | Interactive UI | Free (local) |

### Package Structure

```
arxiv-cosci/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ api/           # FastAPI backend (planned)
â”‚   â”œâ”€â”€ web/           # React frontend (planned)
â”‚   â””â”€â”€ cli/           # Click CLI commands (âœ… done)
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ ingestion/     # Data fetching & parsing
â”‚   â”‚   â”œâ”€â”€ s2_client.py        # Semantic Scholar API
â”‚   â”‚   â”œâ”€â”€ pdf_downloader.py   # arXiv PDF fetch
â”‚   â”‚   â”œâ”€â”€ text_extractor.py   # PyMuPDF + Marker + Grobid
â”‚   â”‚   â”œâ”€â”€ kaggle_loader.py    # Legacy: Kaggle support
â”‚   â”‚   â””â”€â”€ models.py           # Pydantic schemas
â”‚   â”œâ”€â”€ knowledge/     # Graph & vector storage
â”‚   â”‚   â”œâ”€â”€ neo4j_client.py     # Neo4j async client
â”‚   â”‚   â””â”€â”€ chromadb_client.py  # ChromaDB wrapper
â”‚   â”œâ”€â”€ ai/            # LLM pipelines
â”‚   â”‚   â”œâ”€â”€ factory.py              # Provider selection
â”‚   â”‚   â”œâ”€â”€ gemini_client.py        # Gemini API
â”‚   â”‚   â”œâ”€â”€ groq_client.py          # Groq fallback
â”‚   â”‚   â”œâ”€â”€ ollama_client.py        # Local LLM
â”‚   â”‚   â”œâ”€â”€ llm_base.py             # Base interface
â”‚   â”‚   â”œâ”€â”€ summarizer.py           # Paper summarization
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py     # Entity extraction
â”‚   â”‚   â””â”€â”€ citation_classifier.py  # Intent classification
â”‚   â””â”€â”€ ml/            # Machine learning models
â”‚       â”œâ”€â”€ link_predictor.py   # GraphSAGE training
â”‚       â”œâ”€â”€ embeddings.py       # Batch embedding generation
â”‚       â””â”€â”€ hypothesis_gen.py    # Hypothesis generation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # Downloaded PDFs & metadata
â”‚   â”œâ”€â”€ processed/     # Parsed JSON papers
â”‚   â””â”€â”€ models/        # Trained GNN checkpoints
â”œâ”€â”€ docker/            # Docker compose & configs
â”œâ”€â”€ tests/             # pytest test suite
â”œâ”€â”€ docs/              # Documentation
â”œâ”€â”€ pyproject.toml     # Poetry dependencies
â”œâ”€â”€ .env.example       # Environment template
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ CLAUDE.md      # Project-specific instructions
â”‚   â”œâ”€â”€ plan.md        # This file
â”‚   â””â”€â”€ architecture-analysis.md
â””â”€â”€ README.md          # Project overview
```

---

## 4. Data Flow Diagrams

### Current (Phase 1-4) API-First Flow

```
User Request
    â†“
arxiv-cosci CLI (fetch/summarize/extract/search)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Semantic Scholar API (10 req/sec)       â”‚
â”‚ - Paper metadata (title, abstract)      â”‚
â”‚ - Authors, publish date, categories     â”‚
â”‚ - Citation counts, influence scores     â”‚
â”‚ - AI-generated TLDRs                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Pydantic Validation (PaperMetadata)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gemini API (60 RPM Flash)               â”‚
â”‚ - Summarization (brief/standard/detailed)â”‚
â”‚ - Entity extraction (methods, datasets) â”‚
â”‚ - Citation intent classification       â”‚
â”‚ - Structured JSON output                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚              â”‚
v              v              v
Neo4j      ChromaDB      Local Cache
(Graph)    (Vector)      (JSON)
```

### Future (Phase 2+) Full Pipeline

```
arXiv Paper Repository
    â†“
S2 API Fetch â”€â”€â–º S2 Paper Metadata
                 â”œâ”€â–º Neo4j (Paper node)
                 â””â”€â–º Save Raw JSON
    â†“
Optional: arXiv PDF Download
    â†“
PDF Parsing (Marker + Grobid + PyMuPDF)
    â†“
Gemini Analysis Pipeline
    â”œâ”€â–º Summarize â”€â”€â–º Neo4j
    â”œâ”€â–º Extract Entities â”€â”€â–º Concept nodes
    â”œâ”€â–º Classify Citations â”€â”€â–º Citation edges
    â””â”€â–º Generate Embeddings â”€â”€â–º ChromaDB
    â†“
Link Prediction (GraphSAGE on citation network)
    â†“
Hypothesis Generation (Gemini + LangChain)
    â†“
API Layer (FastAPI)
    â†“
Frontend (React)
```

---

## 5. CLI Commands Reference

### Data Fetching

```bash
# Fetch single paper
poetry run arxiv-cosci fetch 2401.12345

# Fetch multiple papers with JSON output
poetry run arxiv-cosci fetch 2401.12345 2402.13579 \
  -o data/papers.json \
  --with-citations \
  --with-references

# Search papers (semantic)
poetry run arxiv-cosci search "quantum error correction" --limit 20
```

### Analysis

```bash
# Summarize paper
poetry run arxiv-cosci summarize 2401.12345 --level detailed

# Extract entities
poetry run arxiv-cosci extract 2401.12345 --use-llm

# Check AI system
poetry run arxiv-cosci ai-check
```

### Database

```bash
# Initialize Neo4j schema
poetry run arxiv-cosci init-db

# Show statistics
poetry run arxiv-cosci db-stats

# Ingest papers to knowledge graph
poetry run arxiv-cosci ingest -i data/processed \
  --to-neo4j --to-chroma
```

### System Health

```bash
# Check dependencies
poetry run arxiv-cosci check

# Show metadata statistics
poetry run arxiv-cosci stats data/raw/arxiv-metadata.json
```

---

## 6. Configuration & Environment

### Required Environment Variables

```bash
# Gemini API (required for AI analysis)
export GEMINI_API_KEY="your_gemini_key"

# Semantic Scholar API (optional: 10 req/sec vs 1 req/sec)
export S2_API_KEY="your_s2_key"

# LLM Provider selection (default: ollama)
export LLM_PROVIDER="gemini"  # or "groq" or "ollama"

# Optional: Groq API key (fallback provider)
export GROQ_API_KEY="your_groq_key"

# Neo4j connection
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USER="neo4j"
export NEO4J_PASSWORD="password"
```

### Local Database Setup

```bash
# Start Neo4j with Docker
docker compose up -d neo4j

# Open Neo4j browser
open http://localhost:7474

# Run schema initialization
poetry run arxiv-cosci init-db
```

---

## 7. Success Metrics & KPIs

### Phase-Based Success Criteria

| Phase | Metric | Target | Current |
|-------|--------|--------|---------|
| 1 | Papers fetched | 100+ | âœ… Can fetch |
| 2 | Parsing accuracy | 90%+ | â³ TBD |
| 3 | Query latency | <500ms | â³ TBD |
| 4 | Entity extraction | 80%+ accuracy | âœ… ~80% |
| 5 | Link prediction | AUC >0.8 | â³ TBD |
| 6 | UI responsiveness | <2s page load | â³ TBD |

### Data Quality Metrics

| Metric | Target | Notes |
|--------|--------|-------|
| S2 API availability | 99.9% | Cached in Neo4j |
| Gemini success rate | 95%+ | Exponential backoff on failure |
| Entity extraction precision | 85%+ | Validated via human review |
| Graph query accuracy | 100% | Cypher correctness |

### Cost Metrics (Target: $0)

| Component | Free Tier | Usage | Cost |
|-----------|-----------|-------|------|
| Semantic Scholar | 10 req/sec | 100k papers / 33hrs | $0 |
| Gemini Flash | 60 RPM, 1M tokens/day | ~500 papers/month | $0 |
| Neo4j | Community self-hosted | Unlimited | $0 |
| ChromaDB | Embedded | Unlimited | $0 |
| **Total** | | | **$0** |

---

## 8. Risk Management

### Identified Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|-----------|
| S2 API rate limiting | Medium | High | Exponential backoff, caching in Neo4j |
| Gemini quota exceeded | Low | High | Groq fallback, queue batch requests |
| PDF parsing failures | Medium | Medium | Fallback chain (Markerâ†’PyMuPDF) |
| Entity extraction accuracy | Medium | Medium | Domain-specific prompts + human review |
| Neo4j performance | Low | High | Index on arxiv_id, query caching |
| LaTeX reconstruction | Medium | Medium | Store original .tex when available |
| Semantic search quality | Medium | Low | Use multiple embedding models |

### Quality Assurance Plan

| Activity | Frequency | Responsibility |
|----------|-----------|-----------------|
| Unit tests | Per commit | CI/CD (pytest) |
| API rate limiting tests | Weekly | Manual + monitoring |
| Entity extraction validation | Monthly | Human review of samples |
| Graph query performance | Monthly | Benchmark suite |
| Entity extraction benchmarking | Quarterly | Full corpus validation |

---

## 9. Key Implementation Files

| File | Status | Purpose |
|------|--------|---------|
| `packages/ingestion/s2_client.py` | âœ… Done | Semantic Scholar async client with retry logic |
| `packages/ai/factory.py` | âœ… Done | LLM provider selection (Gemini/Groq/Ollama) |
| `packages/ai/gemini_client.py` | âœ… Done | Gemini API wrapper with structured output |
| `packages/knowledge/neo4j_client.py` | âœ… Done | Neo4j async driver + schema initialization |
| `packages/knowledge/chromadb_client.py` | âœ… Done | ChromaDB embedding storage |
| `apps/cli/main.py` | âœ… Partial | CLI commands (12/13 implemented) |
| `.claude/CLAUDE.md` | âœ… Done | Project-specific instructions |
| `.claude/architecture-analysis.md` | âœ… Done | API vs Local comparison |
| `pyproject.toml` | âœ… Done | Poetry dependencies (semanticscholar added) |
| `tests/test_ai.py` | âœ… Done | LLM client tests |
| `tests/test_ingestion.py` | âœ… Done | S2 client + model tests |

---

## 10. Testing Strategy

### Current Test Coverage

```bash
poetry run pytest tests/ -v

Results: 40 passed, 3 skipped
- test_ai.py: Multi-provider LLM testing
- test_ingestion.py: S2 client + Pydantic validation
- test_gemini_client.py: Structured output parsing
```

### Planned Tests

| Module | Tests | Status |
|--------|-------|--------|
| Neo4j client | Cypher generation, connection pooling | â³ TBD |
| ChromaDB | Embedding generation, search quality | â³ TBD |
| PDF parser | Markdown output structure | â³ TBD |
| Entity extractor | Recall, precision on known entities | â³ TBD |
| GraphSAGE | Link prediction evaluation | â³ TBD |
| FastAPI endpoints | HTTP status codes, response schema | â³ TBD |

---

## 11. Documentation

### Current Documentation

- âœ… `README.md` - Project overview + quick start
- âœ… `.claude/CLAUDE.md` - AI assistant instructions
- âœ… `.claude/architecture-analysis.md` - Design decision rationale
- âœ… `.claude/plan.md` - This comprehensive plan

### Missing Documentation

- â³ API endpoint documentation (once FastAPI built)
- â³ GraphQL schema documentation
- â³ Cypher query patterns guide
- â³ Entity extraction rule book
- â³ Deployment guide (Docker, K8s)
- â³ Contributing guidelines

---

## 12. Next Immediate Actions

### For Session N+1 (Resume point)

**Priority 1 (Critical Path):**
1. Fix Gemini SDK deprecation warning (migrate google.generativeai â†’ google.genai)
2. Implement Phase 2: PDF parsing integration (Marker + Grobid)
3. Build Phase 3: Neo4j ingestion pipeline for 1000+ papers
4. Add batch processing to speed up paper analysis

**Priority 2 (Enablement):**
5. Expand test coverage for Neo4j + ChromaDB
6. Document API endpoints as FastAPI is built
7. Add observability (logging + monitoring)

**Priority 3 (Enhancement):**
8. Optimize graph queries with caching
9. Implement structured logging with structlog
10. Add Docker Compose orchestration

### Known Bugs/TODOs

- [ ] Google generativeai package is deprecated (FutureWarning) â†’ migrate to google.genai
- [ ] LangChain orchestration marked "In Progress" (Phase 4) â†’ complete async workflow
- [ ] Batch processing pipeline pending (Phase 4) â†’ implement async batch jobs

---

## 13. Hardware & Deployment

### Development Environment

**Current:** M1/M2 Mac (16GB+ RAM recommended)
- Ollama: Local LLM fallback
- sentence-transformers: Local embeddings
- Neo4j: Docker container

### Deployment Targets

**Docker Compose (Recommended):**
- Neo4j service + volumes
- ChromaDB persistent storage
- Ollama service (optional, for local LLM)

**Production (Future):**
- Kubernetes: Neo4j cluster, API service
- Cloud storage: S3/GCS for large embedding backups
- CDN: FastAPI â†’ Cloudflare

### Scalability Roadmap

| Stage | Scale | Hardware | Deployment |
|-------|-------|----------|------------|
| **Current** | 1k papers | M1/M2 + Docker | Local machine |
| **Phase 3** | 10k papers | M1/M2 Pro + 32GB | Docker Compose |
| **Phase 5** | 100k papers | Kubernetes + Redis | Cloud (GCP/AWS) |
| **Production** | 1M papers | Distributed Neo4j | Multi-region |

---

## 14. Version History

| Date | Version | Change |
|------|---------|--------|
| Jan 9, 2026 | 0.2.0 | **Phase 2 complete:** PDF parsing pipeline (Marker + Grobid + LaTeX + Chunking) |
| Jan 7, 2026 | 0.1.0 | **Architecture migration:** Kaggle â†’ Semantic Scholar API, Ollama â†’ Gemini |
| Jan 4, 2026 | 0.0.4 | **Phase 4 complete:** Multi-provider LLM support implemented |
| Dec 20, 2025 | 0.0.3 | Neo4j + ChromaDB setup |
| Dec 18, 2025 | 0.0.2 | Project scaffolding |
| Dec 1, 2025 | 0.0.1 | Initial plan creation |

---

## 15. References

### API Documentation
- [Semantic Scholar API](https://api.semanticscholar.org/api-docs/)
- [Gemini API](https://ai.google.dev/)
- [Groq API](https://groq.com/product/)
- [arXiv API](https://arxiv.org/help/api/user-manual)
- [Neo4j Documentation](https://neo4j.com/docs/)
- [ChromaDB Docs](https://docs.trychroma.com/)

### Libraries & Tools
- [LangChain](https://langchain.readthedocs.io/)
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)
- [sentence-transformers](https://www.sbert.net/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Click](https://click.palletsprojects.com/)

### Research Papers
- GraphSAGE: [Inductive Representation Learning](https://arxiv.org/abs/1706.02216)
- MTEB Embeddings: [Massive Text Embedding Benchmark](https://huggingface.co/spaces/mteb/leaderboard)

---

**Last Updated:** January 9, 2026
**Project Status:** Active Development
**Maintainer:** Andrew Gibson
**License:** MIT (planned)
